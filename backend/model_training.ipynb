{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f4437c-1682-4d69-aa4b-da5bf747e8c2",
   "metadata": {},
   "source": [
    "# üß≠ Project Title\n",
    "\n",
    "## Zero-Trust Anomaly Detection in Authentication Logs\n",
    "\n",
    "### üí° Objective\n",
    "\n",
    "Detect suspicious login behaviors (e.g., off-hours logins, impossible travel, unusual resource access) using a combination of unsupervised and semi-supervised ML models such as:\n",
    "\n",
    "Isolation Forest\n",
    "\n",
    "Autoencoder (deep learning)\n",
    "\n",
    "One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d158cd3-3429-4402-895c-77bd473ec1d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'runtime_version' from 'google.protobuf' (/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/protobuf/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneClassSVM\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mag_ctx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[32m     25\u001b[39m stacks = threading.local()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_list\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m typing \u001b[38;5;28;01mas\u001b[39;00m npt\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m message\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m full_type_pb2\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/tensorflow/core/framework/attr_value_pb2.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m runtime_version \u001b[38;5;28;01mas\u001b[39;00m _runtime_version\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m symbol_database \u001b[38;5;28;01mas\u001b[39;00m _symbol_database\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'runtime_version' from 'google.protobuf' (/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/protobuf/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765521cb-ff7d-40e4-b417-0dabe33f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/sheltonsimbi/projects/joan-testing/backend/data/auth_logs_raw.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f12cb-7f9e-4c97-8dbe-8ceedf82fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# üß† Apply Zero-Trust Labeling\n",
    "# ------------------------------------------------------\n",
    "# In Zero-Trust, ONLY \"normal\" events are trusted.\n",
    "# Everything else is treated as anomalous (1 = anomaly, 0 = normal)\n",
    "ZERO_TRUST_NORMAL_LABEL = \"normal\"\n",
    "zero_trust_anomaly_labels = sorted(\n",
    "    label for label in df[\"event_label\"].unique() if label != ZERO_TRUST_NORMAL_LABEL\n",
    ")\n",
    "\n",
    "df[\"binary_label\"] = (df[\"event_label\"] != ZERO_TRUST_NORMAL_LABEL).astype(int)\n",
    "\n",
    "print(\"Zero-Trust anomaly labels:\", zero_trust_anomaly_labels)\n",
    "print(df[\"binary_label\"].value_counts())\n",
    "print(\"\\n‚úÖ Zero-Trust labels applied: 0 = Normal, 1 = Anomaly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b924e-b4fd-46ef-95fe-bb3f6f958ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Quick overview\n",
    "df.info()\n",
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fc9a9-721e-4892-8420-adb5fbd2f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binary_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95679fa-534d-4c64-a98e-9c1a52ac61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5da1eb-a9de-4e89-8c52-3b64d901a0e9",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130669c7-5b0e-4ae6-a8d6-efbb27d73ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f334e-6df8-4663-b5c6-6ffe88b3995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Event label distribution\n",
    "plt.figure(figsize=(12,6))\n",
    "df['binary_label'].value_counts().plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Event Labels')\n",
    "plt.xticks(rotation=75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433f2fb-0d81-4669-8a7a-5b00d49cd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Bytes transferred distribution\n",
    "sns.histplot(df['bytes_transferred'], bins=50, kde=True)\n",
    "plt.title('Distribution of Bytes Transferred')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95bf39-d5eb-497f-bb3a-d9293a5cf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Access time distribution\n",
    "df['hour'] = pd.to_datetime(df['access_time'], format='%H:%M:%S').dt.hour\n",
    "sns.histplot(df['hour'], bins=24, kde=False)\n",
    "plt.title('Login Hour Distribution')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355a12f-e240-4efc-8339-7efb1f5dfab9",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 4. Feature Engineering\n",
    "\n",
    "We‚Äôll extract numeric and encoded features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b89f5-cef1-4d96-ac19-b4f356372aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Encode categorical columns\n",
    "categorical_cols = ['user_id', 'device_id', 'ip_address', 'location', 'resource_accessed']\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = encoder.fit_transform(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c3a7-faae-4575-8c02-ae7af8588f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Convert event_label to binary (normal=0, anomaly=1)\n",
    "df['is_anomaly'] = df['binary_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb5cba-7b5a-43e2-ac5d-9df13625a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final feature matrix\n",
    "features = ['user_id', 'device_id', 'ip_address', 'location', \n",
    "            'login_success', 'hour', 'resource_accessed', 'bytes_transferred']\n",
    "X = df[features]\n",
    "y = df['is_anomaly']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de809016-f8d5-4012-be9a-99200297f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34848ee6-288e-4d8e-8e18-09fcc3630568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Split data (even though anomalies are rare)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "expected_contamination = min(max(y_train.mean(), 1e-3), 0.49)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a1a39-72fa-4e6a-af3e-021f48590429",
   "metadata": {},
   "source": [
    "# üß† 6. Model 1 ‚Äî Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60898446-1ab9-40cc-8dd6-75b1e02dc82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Isolation Forest (Zero-Trust friendly)\n",
    "iso = IsolationForest(contamination=expected_contamination, random_state=42)\n",
    "iso.fit(X_train_normal)\n",
    "\n",
    "y_pred_iso = iso.predict(X_test)\n",
    "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # Convert (-1 = anomaly) to (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21f38c-d753-4c17-af2c-ab3fe1d2d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluation\n",
    "print(\"Isolation Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_iso, target_names=[\"normal\", \"anomaly\"]))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_iso), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Isolation Forest Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d2fcd-1f11-45c4-9a50-c61f6fe87cf2",
   "metadata": {},
   "source": [
    "# ü§ñ 7. Model 2 ‚Äî One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a51f87-f71e-477d-a523-41250caab5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: One-Class SVM\n",
    "svm_nu = min(max(expected_contamination, 0.01), 0.5)\n",
    "svm = OneClassSVM(kernel='rbf', gamma=0.001, nu=svm_nu)\n",
    "svm.fit(X_train_normal)  # Train on normal data only\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "y_pred_svm = np.where(y_pred_svm == -1, 1, 0)\n",
    "\n",
    "print(\"One-Class SVM Results:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"normal\", \"anomaly\"]))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"One-Class SVM Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e644a24-ef17-4b8e-aa5e-5c86307cbd03",
   "metadata": {},
   "source": [
    "# üß¨ 8. Model 3 ‚Äî Autoencoder (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748d37c-c26b-418e-8028-b0a8ad3d8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 4\n",
    "\n",
    "autoencoder = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(encoding_dim, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='linear')\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal,\n",
    "    X_train_normal,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d41f4-360d-4f1b-807f-01d73cfeba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Reconstruction error and threshold\n",
    "reconstructions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "y_pred_auto = np.where(mse > threshold, 1, 0)\n",
    "\n",
    "print(\"Autoencoder Results:\")\n",
    "print(classification_report(y_test, y_pred_auto, target_names=[\"normal\", \"anomaly\"]))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_auto), annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title(\"Autoencoder Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde842f-a8e5-4df1-9972-3ae336630bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Compare models\n",
    "models = ['Isolation Forest', 'One-Class SVM', 'Autoencoder']\n",
    "aucs = [\n",
    "    roc_auc_score(y_test, y_pred_iso),\n",
    "    roc_auc_score(y_test, y_pred_svm),\n",
    "    roc_auc_score(y_test, y_pred_auto)\n",
    "]\n",
    "\n",
    "plt.bar(models, aucs, color=['steelblue','seagreen','orange'])\n",
    "plt.title(\"Model AUC Comparison\")\n",
    "plt.ylabel(\"ROC-AUC Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58198dc0-443c-4678-a457-4a7ebc65f2d3",
   "metadata": {},
   "source": [
    "# üß© 19. Fine-Tuning Hyperparameters\n",
    "\n",
    "We‚Äôll perform grid-search-style tuning for contamination, nu, and the Autoencoder‚Äôs threshold percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee08686-2876-40b2-a7dc-f4e2ee5f720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Fine-tuning IsolationForest, OneClassSVM, and Autoencoder threshold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_models(contamination_vals, nu_vals, threshold_percents):\n",
    "    best_results = {}\n",
    "\n",
    "    for c in contamination_vals:\n",
    "        iso = IsolationForest(contamination=c, random_state=42)\n",
    "        iso.fit(X_train_normal)\n",
    "        preds = np.where(iso.predict(X_test) == -1, 1, 0)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        best_results[f\"IsolationForest_c={c}\"] = f1\n",
    "\n",
    "    for n in nu_vals:\n",
    "        svm = OneClassSVM(kernel='rbf', gamma='scale', nu=n)\n",
    "        svm.fit(X_train_normal)\n",
    "        preds = np.where(svm.predict(X_test) == -1, 1, 0)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        best_results[f\"OneClassSVM_nu={n}\"] = f1\n",
    "\n",
    "    recon = autoencoder.predict(X_test)\n",
    "    mse = np.mean(np.power(X_test - recon, 2), axis=1)\n",
    "    for t in threshold_percents:\n",
    "        threshold = np.percentile(mse, t)\n",
    "        preds = np.where(mse > threshold, 1, 0)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        best_results[f\"Autoencoder_thr={t}\"] = f1\n",
    "\n",
    "    return pd.DataFrame(list(best_results.items()), columns=[\"Model_Param\", \"F1_Score\"]).sort_values(\"F1_Score\", ascending=False)\n",
    "\n",
    "tune_results = evaluate_models(\n",
    "    contamination_vals=[0.02, 0.05, 0.1],\n",
    "    nu_vals=[0.01, 0.05, 0.1],\n",
    "    threshold_percents=[90, 95, 99]\n",
    ")\n",
    "\n",
    "tune_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aeacf-e21c-4bd8-9589-5a1b79200c2b",
   "metadata": {},
   "source": [
    "# üîç 20. SHAP-based Feature Importance (Isolation Forest)\n",
    "\n",
    "This helps visualize which features drive anomaly decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd972d-6bff-4d2f-8bad-527f9ef5594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Recreate trained Isolation Forest model ---\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_final = IsolationForest(contamination=expected_contamination, random_state=42)\n",
    "iso_final.fit(X_train_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d257b2-5dca-46a3-9ad8-48e31d349c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP Feature Importance for Isolation Forest ---\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build explainer on your trained model using anomaly scores\n",
    "sample_size = min(500, len(X_train_normal))\n",
    "background = X_train_normal[:sample_size]\n",
    "\n",
    "explainer = shap.KernelExplainer(iso_final.decision_function, background)\n",
    "shap_values = explainer.shap_values(X_test[:sample_size])\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "# Summarize feature importance\n",
    "shap.summary_plot(shap_values, X_test[:sample_size], feature_names=features)\n",
    "plt.title(\"SHAP Feature Importance - Isolation Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085dffc0-ed15-453a-b8f2-75e1fe5dc452",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 21. REST API Deployment with FastAPI + Kafka (Simulation)\n",
    "\n",
    "Below is a lightweight FastAPI service that exposes /predict for real-time anomaly scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ac2fa-8afc-4f32-9aab-2fc9b5402989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: FastAPI REST endpoint (run separately with uvicorn)\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI(title=\"Zero-Trust Anomaly Detector\")\n",
    "\n",
    "# save models\n",
    "joblib.dump(iso_final, \"isoforest_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: dict):\n",
    "    x = np.array(data[\"features\"]).reshape(1, -1)\n",
    "    x_scaled = joblib.load(\"scaler.pkl\").transform(x)\n",
    "    pred = joblib.load(\"isoforest_model.pkl\").predict(x_scaled)\n",
    "    result = \"anomaly\" if pred[0] == -1 else \"normal\"\n",
    "    return {\"prediction\": result}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa5983-1299-4691-aec3-a3d1671e7c78",
   "metadata": {},
   "source": [
    "# üìä 23. Zero-Trust Dashboard (Streamlit)\n",
    "\n",
    "This adds an interactive monitoring dashboard.\n",
    "\n",
    "Create a new file dashboard/app.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b3491-f794-4758-8210-ac826c517c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23 (dashboard/app.py)\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "st.title(\"üîí Zero-Trust Anomaly Detection Dashboard\")\n",
    "\n",
    "iso_model = joblib.load(\"isoforest_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "uploaded = st.file_uploader(\"Upload authentication logs (CSV)\", type=[\"csv\"])\n",
    "\n",
    "if uploaded:\n",
    "    df = pd.read_csv(uploaded)\n",
    "    X = scaler.transform(df[features])\n",
    "    preds = np.where(iso_model.predict(X) == -1, \"Anomaly\", \"Normal\")\n",
    "    df[\"Prediction\"] = preds\n",
    "    st.dataframe(df)\n",
    "    st.bar_chart(df[\"Prediction\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d5f4c-fd64-47ef-adc8-dd0d11d7bb9e",
   "metadata": {},
   "source": [
    "# üïí 24. Temporal Sequence Behavior (User Baseline)\n",
    "\n",
    "We‚Äôll add a user-based average activity tracker to identify deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8ea6d-7f6f-431b-aebb-a989f8400201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Temporal baseline modeling\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "user_hour_mean = df.groupby(['user_id', 'hour'])['bytes_transferred'].mean().reset_index()\n",
    "user_hour_mean.rename(columns={'bytes_transferred':'mean_bytes'}, inplace=True)\n",
    "\n",
    "# Merge baseline back\n",
    "df = df.merge(user_hour_mean, on=['user_id','hour'], how='left')\n",
    "df['deviation_ratio'] = (df['bytes_transferred'] / (df['mean_bytes']+1e-5))\n",
    "\n",
    "sns.histplot(df['deviation_ratio'], bins=50, kde=True)\n",
    "plt.title(\"User Activity Deviation Ratio Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc97da-9a53-4643-abbe-4491b95afdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the API:\n",
    "# conda run -n llms python -m uvicorn anomaly_api:app --host 0.0.0.0 --port 8000\n",
    "\n",
    "# M:246.U8g6x9-6K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ae6c2-8394-48c4-b1b2-1f9124f1477c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
